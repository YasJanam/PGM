{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzp04V01nnxP30eUhGlhRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasJanam/RL1/blob/main/REINFORCE_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMLEVH4vIW64",
        "outputId": "642567b1-d27a-44e6-9fd5-d24545d46206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WexEvEeCIunA",
        "outputId": "f741fe67-316c-4a82-f57a-1ac7395c3e28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.12/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.observation_space.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gai78X3-I12P",
        "outputId": "d1a66d53-58b0-4783-f1f5-c9f87e7d8f3d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
            "Discrete(2)\n",
            "(4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNet(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim,hidden_dim=128):\n",
        "    super(PolicyNet, self).__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(state_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, action_dim),\n",
        "        nn.Softmax(dim=-1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc(x)"
      ],
      "metadata": {
        "id": "vj8c5r4_JgVg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_returns(rewards, gamma):\n",
        "  G = 0\n",
        "  returns = []\n",
        "  for r in reversed(rewards):\n",
        "    G = r + gamma * G\n",
        "    returns.insert(0,G)\n",
        "  returns = torch.tensor(returns)\n",
        "  returns = (returns - returns.mean()) / (returns.std() + 1e-8) # normalization\n",
        "  return returns"
      ],
      "metadata": {
        "id": "rvGezlGZK2yP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getPolicy(env=env,hidden_dim=2):\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.n\n",
        "  policy = PolicyNet(state_dim, action_dim,hidden_dim=hidden_dim)\n",
        "  return policy"
      ],
      "metadata": {
        "id": "-EIVClLczp7t"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainFunc(hidd_dim,lr=0.01,gamma=0.99,episode_num=1000):\n",
        "  policy = getPolicy(hidden_dim=hidd_dim)  # hidd_dim => Policy_Net_hidden_dim\n",
        "  optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
        "  for episode in range(episode_num):\n",
        "    state = env.reset()\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    done = False\n",
        "\n",
        "    # rollout\n",
        "    while not done:\n",
        "      state = torch.tensor(state, dtype=torch.float32)\n",
        "\n",
        "      probs = policy(state)\n",
        "      dist = Categorical(probs)\n",
        "      action = dist.sample()\n",
        "      log_prob = dist.log_prob(action)\n",
        "\n",
        "      next_state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "      log_probs.append(log_prob)\n",
        "      rewards.append(reward)\n",
        "      state = next_state\n",
        "\n",
        "    # policy update\n",
        "    returns = compute_returns(rewards, gamma)\n",
        "\n",
        "    loss = 0\n",
        "    for log_prob, G in zip(log_probs, returns):\n",
        "      loss -= log_prob * G\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_reward = sum(rewards)\n",
        "    if episode % 100 == 0 :\n",
        "      print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "  env.close()\n",
        "  return policy"
      ],
      "metadata": {
        "id": "GKa_6SXzLuOG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### train"
      ],
      "metadata": {
        "id": "N53ml7Xi7cuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "برابر 500 هستش CartPole-v1 بیشترین امتیاز قابل دریافت در\n",
        "\n",
        "طوری بشه که به این امتیاز برسیم و حفظش کنه policy باید"
      ],
      "metadata": {
        "id": "gXPshsJ-7fV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "polic = TrainFunc(hidd_dim=3,lr=0.01,gamma=0.99,episode_num=1000)  # lr -> 0.01"
      ],
      "metadata": {
        "id": "QoTLN225QCtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441653e1-0592-473a-c95a-93ef3131bfbd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 18.0\n",
            "Episode 100, Total Reward: 23.0\n",
            "Episode 200, Total Reward: 55.0\n",
            "Episode 300, Total Reward: 222.0\n",
            "Episode 400, Total Reward: 249.0\n",
            "Episode 500, Total Reward: 486.0\n",
            "Episode 600, Total Reward: 500.0\n",
            "Episode 700, Total Reward: 500.0\n",
            "Episode 800, Total Reward: 500.0\n",
            "Episode 900, Total Reward: 500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "polic = TrainFunc(hidd_dim=3,lr=0.001,gamma=0.99,episode_num=1000)  # lr -> 0.001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCGlPWsN1vpL",
        "outputId": "6a8a937b-e37e-4613-9458-a3694f108a6d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 20.0\n",
            "Episode 100, Total Reward: 12.0\n",
            "Episode 200, Total Reward: 20.0\n",
            "Episode 300, Total Reward: 13.0\n",
            "Episode 400, Total Reward: 12.0\n",
            "Episode 500, Total Reward: 17.0\n",
            "Episode 600, Total Reward: 54.0\n",
            "Episode 700, Total Reward: 16.0\n",
            "Episode 800, Total Reward: 20.0\n",
            "Episode 900, Total Reward: 26.0\n"
          ]
        }
      ]
    }
  ]
}