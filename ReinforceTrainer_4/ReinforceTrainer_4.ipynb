{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MgJB3wl0VOWG",
        "7CQaUH2UUblD",
        "_gPZSpyyUJrz",
        "Hny6WzreYvG-",
        "UGjMFS4uDIdI",
        "BU0cpaxhwttr"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOJLk7c3NqP3NmEZczTJm6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasJanam/RL1/blob/main/ReinforceTrainer_4/ReinforceTrainer_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Requirements**"
      ],
      "metadata": {
        "id": "h3lmZJFvUjJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### install modules"
      ],
      "metadata": {
        "id": "EocAeGa2BHgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y gym gymnasium box2d box2d-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrKomBV3WMN8",
        "outputId": "26a92633-a231-4eb4-9e9e-12813ca59256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping gym as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping gymnasium as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping box2d as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping box2d-py as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium==0.29.1\n",
        "!pip install swig\n",
        "!pip install box2d-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Qm_JjnKxWRyq",
        "outputId": "63c44db2-ec39-4fb6-f8c3-04a3e9c23862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.2.1\n",
            "    Uninstalling gymnasium-1.2.1:\n",
            "      Successfully uninstalled gymnasium-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1\n",
            "Collecting swig\n",
            "  Downloading swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.1.post0\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp312-cp312-linux_x86_64.whl size=2409500 sha256=532ec855ba55df00267c6f99d6a27c226727c1ed53f775166abfbd843262b911\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/3c/ab/b6fd75459cadc56f4a4125d4cb387a708a59ca8589e4cc6b7d\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### imports"
      ],
      "metadata": {
        "id": "SY8JMrT1BE9Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3suNUR6yGEDz"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Environments**"
      ],
      "metadata": {
        "id": "6Y_YMmbbUozT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **CartPole**"
      ],
      "metadata": {
        "id": "8k-e0re9DReU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "MKlvyHIuDReU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **MountainCar**"
      ],
      "metadata": {
        "id": "MgJB3wl0VOWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "حالت : دو بعدی\n",
        "( موقعیت ، سرعت )\n",
        "\n",
        "عمل ها : جلو / عقب / بدون حرکت\n",
        "\n",
        "پاداش :\n",
        "تا وقتی بالا نره، پاداش منفی میگیره\n",
        "\n",
        "هدف : بالا بردن ماشین از تپه"
      ],
      "metadata": {
        "id": "h10jZDcUGYGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MountainCar = gym.make('MountainCar-v0')"
      ],
      "metadata": {
        "id": "4_O9HyGoGOhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(MountainCar.observation_space)\n",
        "print(MountainCar.observation_space.shape)\n",
        "print(MountainCar.action_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMe4Rqj3HPHx",
        "outputId": "e131c8af-0c9a-4b05-ff7a-4686884026d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "(2,)\n",
            "Discrete(3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **LunarLander**"
      ],
      "metadata": {
        "id": "xNr2GWEQVR_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LunarLander = gym.make('LunarLander-v2')"
      ],
      "metadata": {
        "id": "uSbycVfHTilk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **PolicyNet**"
      ],
      "metadata": {
        "id": "7CQaUH2UUblD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNet(nn.Module):\n",
        "  def __init__(self,state_dim,action_dim,hidden_dim=100):\n",
        "    super(PolicyNet,self).__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(state_dim,hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim,action_dim),\n",
        "        nn.Softmax(dim=-1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.fc(x)"
      ],
      "metadata": {
        "id": "X3b35rHvHjLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ReinforceTrainer**"
      ],
      "metadata": {
        "id": "_gPZSpyyUJrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReinforceTrainer(nn.Module):\n",
        "  def __init__(self,env,hidden_dim=128,lr=0.01,gamma=0.99,episodes_num=1000,log_interval=100,max_rewards=float('inf')):\n",
        "    super().__init__()\n",
        "    self.env = env\n",
        "    self.lr = lr\n",
        "    self.log_interval = log_interval\n",
        "    self.policy_hidden_dim = hidden_dim\n",
        "    self.episode_num = episodes_num\n",
        "    self.gamma = gamma\n",
        "    self.max_rewards = max_rewards\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    self.policy = PolicyNet(state_dim=state_dim,action_dim=action_dim,hidden_dim=self.policy_hidden_dim).to(device)\n",
        "    self.optimizer = optim.Adam(self.policy.parameters(),lr=self.lr)\n",
        "\n",
        "  def compute_returns(self,rewards):\n",
        "    G = 0\n",
        "    returns = []\n",
        "    for r in reversed(rewards):\n",
        "      G = r + self.gamma * G\n",
        "      returns.insert(0,G)\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # normalization\n",
        "    return returns.to(device)\n",
        "\n",
        "  def train(self):\n",
        "    for episode in range(self.episode_num):\n",
        "      state, _ = self.env.reset()\n",
        "      log_probs = []\n",
        "      rewards = []\n",
        "      done = False\n",
        "\n",
        "      # rollout\n",
        "      while not done:\n",
        "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
        "\n",
        "        probs = self.policy(state)\n",
        "        dist = Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        next_state, reward, terminated, truncated, info = self.env.step(action.item())\n",
        "        done = terminated or truncated\n",
        "\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(reward)\n",
        "        state = next_state\n",
        "\n",
        "      # log\n",
        "      total_reward = sum(rewards)\n",
        "      if episode % self.log_interval == 0 or episode == (self.episode_num -1) :\n",
        "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "      # early stopping\n",
        "      if total_reward >= self.max_rewards:\n",
        "        print(f\"Reached max reward {total_reward:.2f}, stopping training!\")\n",
        "        break\n",
        "\n",
        "      # policy update\n",
        "      returns = self.compute_returns(rewards)\n",
        "\n",
        "      loss = 0\n",
        "      for log_prob, G in zip(log_probs, returns):\n",
        "        loss -= log_prob * G\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      self.env.close()"
      ],
      "metadata": {
        "id": "eBOvpZw1ODZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DONE** ( gymnasium-done ) :\n",
        "\n",
        "terminated :\n",
        "اپیزود به خاطر رسیدن به هدف یا شکست تموم شده\n",
        "\n",
        "truncated :\n",
        "اپیزود به خاطر محدودیت زمانی قطع شده\n",
        "\n",
        "done = terminated or truncated"
      ],
      "metadata": {
        "id": "aMY0NEaTOCMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**سیگنال پاداش معنی دار** :\n",
        "\n",
        "الگوریتم رینفورس برای یاد گیری نیاز به پاداش معنی دار داره ! یعنی یه تفاوتی بین اکشن های خوب و بد باشه. چون اینجوری کار میکنه که بر حسب پاداش هایی که میگیره به سمت هدف حرکت کنه\n",
        "\n",
        "**MountainCar** :\n",
        "\n",
        "در این محیط تا وقتی عامل به هدف نرسد تمام پاداش ها یکی هستند و برابر -1 اند. یعنی پاداش یک سیگنال معنی دار که عامل را به سمت هدف ببرد نیست. در نتیجه الگوریتم رینفورس روی این محیط کارایی ندارد. در ادامه این موضوع قابل مشاهده است\n",
        "\n"
      ],
      "metadata": {
        "id": "UsHOA5DuS5Rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Test**"
      ],
      "metadata": {
        "id": "Hny6WzreYvG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(policy,env):\n",
        "  state, _ = env.reset()\n",
        "  done = False\n",
        "  total_reward = 0\n",
        "\n",
        "  while not done:\n",
        "      state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "\n",
        "      # انتخاب اکشن با policy\n",
        "      with torch.no_grad():  # gradient محاسبه نشه\n",
        "          probs = policy(state_tensor)\n",
        "      dist = torch.distributions.Categorical(probs)\n",
        "      action = dist.sample().item()\n",
        "\n",
        "      # اعمال اکشن\n",
        "      next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "      done = terminated or truncated\n",
        "\n",
        "      # جمع‌آوری پاداش\n",
        "      total_reward += reward\n",
        "      state = next_state\n",
        "\n",
        "  return total_reward\n"
      ],
      "metadata": {
        "id": "Dd-naxnPYyAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tests(policy,env,num_tests):\n",
        "  total_rewards = []\n",
        "  for _ in range(num_tests):\n",
        "    total_reward = test(policy,env)\n",
        "    total_rewards.append(total_reward)\n",
        "  rewards = [float(x) for x in total_rewards]\n",
        "  return rewards"
      ],
      "metadata": {
        "id": "hPjUF-PjKHT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **tarin**"
      ],
      "metadata": {
        "id": "nAPA9x4bUS9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### CartPole"
      ],
      "metadata": {
        "id": "UGjMFS4uDIdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cp_trainer = ReinforceTrainer(env=cartpole,hidden_dim=15,lr=0.001,gamma=0.99,episodes_num=1000)\n",
        "cp_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_ntOC-bDM4p",
        "outputId": "46935c9f-42d2-46d5-ae09-79fb377f997f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 24.0\n",
            "Episode 100, Total Reward: 12.0\n",
            "Episode 200, Total Reward: 34.0\n",
            "Episode 300, Total Reward: 21.0\n",
            "Episode 400, Total Reward: 47.0\n",
            "Episode 500, Total Reward: 89.0\n",
            "Episode 600, Total Reward: 63.0\n",
            "Episode 700, Total Reward: 38.0\n",
            "Episode 800, Total Reward: 41.0\n",
            "Episode 900, Total Reward: 270.0\n",
            "Episode 999, Total Reward: 350.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### MountainCar"
      ],
      "metadata": {
        "id": "BU0cpaxhwttr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mc_trainer = ReinforceTrainer(env=MountainCar,hidden_dim=128,lr=0.003,gamma=0.99,episodes_num=500,log_interval=100)\n",
        "mc_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GePRBRmYJsuA",
        "outputId": "1d179f80-3fb8-4f57-8df4-7794a3140fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: -200.0\n",
            "Episode 100, Total Reward: -200.0\n",
            "Episode 200, Total Reward: -200.0\n",
            "Episode 300, Total Reward: -200.0\n",
            "Episode 400, Total Reward: -200.0\n",
            "Episode 499, Total Reward: -200.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "برابر 200 است rollout حداکثر تعداد استپ در یک\n",
        "\n",
        "ملاحظه میکنید که پاداش کل تمام اپیزود ها برابر 200- شده است\n",
        "\n",
        "شده اند truncate ها  rollout این یعنی تمام"
      ],
      "metadata": {
        "id": "Qg-FH-mRUozC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LunarLander"
      ],
      "metadata": {
        "id": "s-CUyfyLwsAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = ReinforceTrainer(env=LunarLander,hidden_dim=50,lr=0.006,gamma=0.99,episodes_num=2000,log_interval=80)\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tau5NQfQx73",
        "outputId": "358b9a93-921d-41d0-983c-762f6362a07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: -53.22990772696299\n",
            "Episode 80, Total Reward: -71.50219906093626\n",
            "Episode 160, Total Reward: -27.5168907254766\n",
            "Episode 240, Total Reward: -2.2126424281705965\n",
            "Episode 320, Total Reward: 79.14784516288864\n",
            "Episode 400, Total Reward: 288.1995739594064\n",
            "Episode 480, Total Reward: 25.65213221542885\n",
            "Episode 560, Total Reward: 283.59043685217114\n",
            "Episode 640, Total Reward: 240.02758369313892\n",
            "Episode 720, Total Reward: 280.8945688418088\n",
            "Episode 800, Total Reward: -31.120080271048337\n",
            "Episode 880, Total Reward: 265.86016334329076\n",
            "Episode 960, Total Reward: 296.78013480048537\n",
            "Episode 1040, Total Reward: 256.0518604122203\n",
            "Episode 1120, Total Reward: 253.05118451874563\n",
            "Episode 1200, Total Reward: -5.715695042433794\n",
            "Episode 1280, Total Reward: 239.9295319837459\n",
            "Episode 1360, Total Reward: 214.14864132132612\n",
            "Episode 1440, Total Reward: 186.44925894713276\n",
            "Episode 1520, Total Reward: 300.8294054036445\n",
            "Episode 1600, Total Reward: 274.1881272949603\n",
            "Episode 1680, Total Reward: 259.79482792864303\n",
            "Episode 1760, Total Reward: 315.11661398348974\n",
            "Episode 1840, Total Reward: 257.75648585183404\n",
            "Episode 1920, Total Reward: 270.07742841177765\n",
            "Episode 1999, Total Reward: 251.55133080110133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = ReinforceTrainer(env=LunarLander,hidden_dim=18,lr=0.005,gamma=0.99,episodes_num=2500,log_interval=150,max_rewards=300)\n",
        "trainer.train()"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haw4PfeGJxCP",
        "outputId": "731f08e2-c5a4-4740-f4c7-f6735a22412e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: -230.7038690052147\n",
            "Episode 150, Total Reward: -71.84500730422391\n",
            "Episode 300, Total Reward: 10.499415628912885\n",
            "Episode 450, Total Reward: 32.67165617267992\n",
            "Episode 600, Total Reward: 102.42998149664243\n",
            "Episode 750, Total Reward: 126.93486553183588\n",
            "Episode 900, Total Reward: 167.79275608654623\n",
            "Episode 1050, Total Reward: 131.97596996515577\n",
            "Episode 1200, Total Reward: 31.478092511944794\n",
            "Episode 1350, Total Reward: 37.563974672560676\n",
            "Episode 1500, Total Reward: 150.0603448661049\n",
            "Episode 1650, Total Reward: 34.741153431294265\n",
            "Episode 1800, Total Reward: 43.35619766601775\n",
            "Episode 1950, Total Reward: 247.83937042013017\n",
            "Episode 2100, Total Reward: 101.761291494854\n",
            "Episode 2250, Total Reward: 245.54383611534453\n",
            "Episode 2400, Total Reward: 121.17526064434972\n",
            "Episode 2499, Total Reward: 130.06700893816534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = tests(trainer.policy,LunarLander,10)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tEm9xRPTb_V",
        "outputId": "fc99c0c9-7c45-4669-8f62-e9a819762075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[130.17978966071044,\n",
              " 133.21017342542714,\n",
              " 127.35077283499963,\n",
              " 224.66911780281038,\n",
              " 242.53399819138812,\n",
              " 210.18660229356198,\n",
              " 147.67705959752644,\n",
              " 193.74221652603677,\n",
              " 262.3148771569899,\n",
              " 65.51015671118907]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = ReinforceTrainer(env=LunarLander,hidden_dim=30,lr=0.003,gamma=0.99,episodes_num=3000,log_interval=150,max_rewards=300)\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dkvlXUFSYMu",
        "outputId": "11ffd4b6-d9dc-4cac-a599-e03b6a3d77f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0, Total Reward: -107.94190874546008\n",
            "Episode 150, Total Reward: -184.6184893474179\n",
            "Episode 300, Total Reward: -341.1119807891307\n",
            "Episode 450, Total Reward: -107.46065396086138\n",
            "Episode 600, Total Reward: -204.50831404637873\n",
            "Episode 750, Total Reward: 56.172122688986235\n",
            "Episode 900, Total Reward: 46.205336856350954\n",
            "Episode 1050, Total Reward: -199.40439431398102\n",
            "Episode 1200, Total Reward: 68.70085134722672\n",
            "Episode 1350, Total Reward: 239.4702302677389\n",
            "Episode 1500, Total Reward: 148.86400117910765\n",
            "Episode 1650, Total Reward: 21.03401594737157\n",
            "Episode 1800, Total Reward: 63.621154168926495\n",
            "Episode 1950, Total Reward: 34.590147386591156\n",
            "Episode 2100, Total Reward: 48.388518283107686\n",
            "Episode 2250, Total Reward: 163.28472630734692\n",
            "Episode 2400, Total Reward: 116.11142987434333\n",
            "Episode 2550, Total Reward: 172.76827586634118\n",
            "Episode 2700, Total Reward: 102.33545557140775\n",
            "Episode 2850, Total Reward: 10.99155536487065\n",
            "Episode 2999, Total Reward: 172.30461738394482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = tests(trainer.policy,LunarLander,10)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhPuXVPoeiA7",
        "outputId": "18fffbfd-f746-4477-94ab-17aa82c7abf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[140.4280540498257,\n",
              " 181.28223141047064,\n",
              " 255.57986375522742,\n",
              " 3.6549266744151083,\n",
              " 117.25934289255697,\n",
              " 152.5821752521934,\n",
              " 169.6383866076964,\n",
              " 147.9446069529386,\n",
              " 144.59193761266326,\n",
              " 161.65569661646225]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}