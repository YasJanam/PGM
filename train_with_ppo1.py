# -*- coding: utf-8 -*-
"""train_with_ppo1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sxoP7alQbQqK73HoSYYrJf_yIahRPnzr
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer ,AutoModelForCausalLM
from torch.optim import Adam
import numpy as np
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

#  value head
class DialoGPTWithValueHead(nn.Module):
  def __init__(self,model_name):
    super().__init__()
    self.model = AutoModelForCausalLM.from_pretrained(model_name)
    hidden_size = self.model.config.n_embd
    self.value_head = nn.Linear(hidden_size,1)

  def forward(self,input_ids,attention_mask=None,labels=None):
    outputs = self.model(input_ids,attention_mask = attention_mask,labels=labels,output_hidden_states=True)
    last_hidden_state = outputs.hidden_states[-1]
    values = self.value_head(last_hidden_state).squeeze(-1)
    return outputs,values

# ppotrainer
class PPOTrainer:
  def __init__(self,model,clip_epsilon=0.2,gamma=0.99,lam=0.95):
    self.model = model
    self.optimizer = Adam(model.parameters(),lr=1e-5)
    self.clip_epsilon = clip_epsilon
    self.gamma = gamma
    self.lam = lam
    self.mse_loss = nn.MSELoss()

  def compute_advantages(self,rewards,values):
    advantages = []
    for r,v in zip(rewards,values):
      adv = r-v
      advantages.append(adv)
    return torch.tensor(advantages,dtype=torch.float32).to(device)

  def update(self,queries,responses,old_log_probs,rewards,values):
    self.model.train()

    input_ids = torch.cat([queries,responses],dim=-1)
    attention_mask = (input_ids != tokenizer.pad_token_id).float().to(device)

    outputs,new_values = self.model(input_ids,attention_mask=attention_mask,labels=input_ids)
    logits = outputs.logits

    response_start = queries.shape[1]
    logits = logits[:,response_start-1:-1]
    log_probs = torch.log_softmax(logits,dim=-1)
    selected_log_probs = torch.gather(log_probs,2,responses.unsqueeze(-1)).squeeze(-1)

    ratios = torch.exp(selected_log_probs-old_log_probs.detach())
    advantages = self.compute_advantages(rewards,values)

    surr1 = ratios*advantages
    surr2 = torch.clamp( ratios,1.0 - self.clip_epsilon ,1.0 + self.clip_epsilon )*advantages
    policy_loss = -torch.min(surr1,surr2).mean()

    value_targets = torch.tensor(rewards,dtype=torch.float32).to(device)
    value_estimates = torch.stack([v[-1] for v in new_values],dim=0)
    value_loss = self.mse_loss(value_estimates,value_targets)

    entropy_loss = -torch.mean(torch.sum(torch.exp(log_probs)*log_probs,dim=-1))

    loss = policy_loss + 0.5*value_loss - 0.01*entropy_loss

    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()

    return loss.item()

model = DialoGPTWithValueHead(model_name).to(device)
ppo_trainer = PPOTrainer(model)

def generate_response(prompt,max_length=50):
  inputs = tokenizer(prompt,return_tensors="pt").to(device)
  outputs = model.model.generate(
      **inputs,
      max_length = max_length,
      do_sample=True,
      top_k=50,
      top_p=0.95,
      pad_token_id = tokenizer.eos_token_id
  )
  return tokenizer.decode(outputs[0],skip_special_tokens=True)

def calculate_rewards(responses):
  return [len(tokenizer.encode(r)) / 100.0 for r in responses]

prompts =[
    "سلام,چطوری؟",
    "امروز هوا چطوره؟",
    "به نظرت هوش مصنوعی چیه؟",
    "دیروز کجا بودی؟"
]

# train
for epoch in range(3):
  epoch_loss=0
  for prompt in tqdm(prompts,desc=f"Epoch{epoch+1}"):
    response_text = generate_response(prompt)

    query_tokens = tokenizer(prompt,return_tensors="pt",padding=True).to(device)
    response_tokens = tokenizer(response_text,return_tensors="pt",padding=True).to(device)

    input_ids = torch.cat([query_tokens["input_ids"],response_tokens["input_ids"]],dim=-1)
    attention_mask = (input_ids != tokenizer.pad_token_id).float().to(device)

    with torch.no_grad():
      outputs,values = model(input_ids,attention_mask=attention_mask)
      logits = outputs.logits
      log_probs = torch.log_softmax(logits[:,query_tokens["input_ids"].shape[-1]-1:-1],dim=-1)
      selected_log_probs = torch.gather(log_probs,2,response_tokens["input_ids"].unsqueeze(-1)).squeeze(-1)

    reward = calculate_rewards([response_text])[0]
    rewards = [reward]
    values_list = [values[0,-1].item()]

    loss = ppo_trainer.update(query_tokens["input_ids"],response_tokens["input_ids"],selected_log_probs,rewards,values_list)
    epoch_loss += loss

    print(f"Epoch {epoch+1}--> Loss :{epoch_loss/len(prompt):.4f}")

