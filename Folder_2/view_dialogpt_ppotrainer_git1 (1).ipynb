{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVvDcaeArDWD",
    "outputId": "787e001c-16fe-4f17-a9ca-c15b9261a927"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "from gym import Env , spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "248a91415473443283060a5839358011",
      "8d661e75a0f94065bd2411fff1b90be3",
      "dbc4e950f6074f039d337c962e59ca4a",
      "f90d14a88ed44c9baef9cca884c69655",
      "1a6afea5983a4ee0b5c66e0c6e68c8b4",
      "971f79e362294d22ab8018c6f5546ac2",
      "f23b1d897bbc4c68add53134568bbcd4",
      "fce225dc0738409183242b36525b056e",
      "e5e7b55567224e6f84f379a00f2b205d",
      "1631531c785747ec9334422c18a07c17",
      "32cba3c470394bc38c1e2f64ffce9106",
      "f4f0ada9fe3e4842a7d5d79cf9fc5f61",
      "ba14ff77b72845d18e1b52231aac0951",
      "bdea1265be7c4d7eb7b52a1495abb9dc",
      "5c470eaabfea4aed9e10e47129b6c7a7",
      "1012f75c6b0f4e348917c546e71cb7af",
      "6e99727598494ba1b21d657d5abf072e",
      "107e4a25399e4c658af6bf7ae8da5b19",
      "3e0e42f7219946fbb5678a31b924e04d",
      "bffaf1a058d146df871f3171a32edbeb",
      "4c8997503a0c4be09e88666d649a964c",
      "24dafb9432634bfaa071616bd1601795",
      "6cfbde25b04341d0bd89aaac0adb719c",
      "966b9dcf3278420089b274fa784aa042",
      "32936699e7134a8c96ab9cfcba424e79",
      "e26e85b1bb8b480d86de4db7992c8015",
      "e3a35dd294b6407290e713cc08f676c0",
      "805b8d8a857f4bbaa2e6bd10b8c067e9",
      "6a780877f2564727b7de74121fe4aeba",
      "0a55761e552a41039d2772796871a82c",
      "4b54a65fd11b446ca769db91ac5b69a3",
      "45038182df3d4e2abf9cb406826619bc",
      "d59098e3992140758dc92b7a143038d3",
      "24f7ffc4eb0545d5aef8c4921c0ec651",
      "5497747b1c9f4a018520009985d10455",
      "8035aa0bca544420835415aacc8cbac6",
      "050dd508f1f54e7cae146ae54638a64c",
      "2c9f882871014fe4b624c76eec1f597e",
      "02a04d79cc0544e18fef24ac2a86cbd2",
      "fe193e81755043c387c48b9eec6a0851",
      "9b56d789e6214323b0dada50be865915",
      "a891f58d2c5d4d91ad043042d4567215",
      "c94118254f0843d69beccac9c29912d9",
      "b6f03347fb1149348910c424dcf060d2",
      "92a20edc91174bd1969d487677d24470",
      "a1613a33f68746718cf6f4082983cb31",
      "b97d0cba98384f87934c3c0653698f91",
      "5b6aabe026f648f588196cd859b10d8e",
      "283e4f57d871421bb7721505c482acd7",
      "11ea5252140247bab223503005b2f9d7",
      "1d6936834f3f48eea3d1e5433bc5f813",
      "41d440a096e34a26b01e5c68a19656ca",
      "d7ca1f25f8374ac1ba2048b81c0dfe57",
      "7f60cb8226a3435995be2c3016a74d5e",
      "402a4dd7dee44222833b365e2a73be77",
      "2c36e82c84cb4684a36d12361bec3f87",
      "6ceade5b789a413eb4fdbdd0e3b6c406",
      "3739194f97b64b48b81c099d2152e361",
      "20eba71e965b4bcc9181c9ace559f066",
      "06d807d7b2004f94b40c64fadb01e044",
      "9df660b16b534bfd87fd833c64716bb3",
      "5f37dc19fde742e4a9074cb8d0f39a8e",
      "6b34b9d6a76548f69c00fcf09e96e130",
      "dfbfe7dfcca5485b9bedcce6a8b521cc",
      "d7e613e69e86482188d270ed88a32ced",
      "1eae430903e043d1823c16a6416b6f81"
     ]
    },
    "id": "w5afs0c8rK3j",
    "outputId": "e5b3a60a-07fb-4262-9862-3c833458da6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248a91415473443283060a5839358011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f0ada9fe3e4842a7d5d79cf9fc5f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfbde25b04341d0bd89aaac0adb719c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f7ffc4eb0545d5aef8c4921c0ec651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a20edc91174bd1969d487677d24470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c36e82c84cb4684a36d12361bec3f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "policy_model=AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sFcZUU7ZrQ6t",
    "outputId": "b6a613bf-0252-4c08-ff40-1893c7ab3796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(policy_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "z4OvBMEOrNbQ"
   },
   "outputs": [],
   "source": [
    "policy_model=policy_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DWf4dPRarXrF"
   },
   "outputs": [],
   "source": [
    "#---Environment---\n",
    "class ChatbotEnv(Env):\n",
    "  def __init__(self):\n",
    "    super(ChatbotEnv,self).__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.model=policy_model\n",
    "    self.action_space=spaces.Discrete(self.tokenizer.vocab_size)\n",
    "    self.observation_space=spaces.Box(low=np.inf,high=np.inf,shape=(self.model.config.hidden_size,),dtype=np.float32)\n",
    "    self.conversation_history=[]\n",
    "\n",
    "  def reset(self):\n",
    "    self.conversation_history =[]\n",
    "    return self._get_observation()\n",
    "\n",
    "  def step(self,action):\n",
    "    response = self._decode_action(action)\n",
    "    self.conversation_history.append(response)\n",
    "    reward = self._calculate_reward(response)\n",
    "    done = len(self.conversation_history) >=5\n",
    "    obs = self._get_observation()\n",
    "    return obs , reward, done, {}\n",
    "\n",
    "  def _decode_action(self,action_token_id):\n",
    "    return self.tokenizer.decode([action_token_id],skip_special_tokens=True)\n",
    "\n",
    "  def _get_observation(self):\n",
    "    if not self.conversation_history:\n",
    "      return np.zeros(self.observation_space.shape,dtype=np.float32)\n",
    "    text = \" \".join(self.conversation_history)\n",
    "    inputs = self.tokenizer(text,return_tensors=\"pt\",truncation=True,max_length=512).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "      outputs = policy_model.transformer(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze(0).detach().cpu().numpy().astype(np.float32).flatten()\n",
    "\n",
    "  def _calculate_reward(self,response):\n",
    "    return len(response.split()) / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feBwbvdarc4f",
    "outputId": "3241fe10-02f9-454b-fb7c-68f4944f342c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation shape:  (768,)\n"
     ]
    }
   ],
   "source": [
    "#---sample---\n",
    "env_sample = ChatbotEnv()\n",
    "obs = env_sample.reset()\n",
    "print(\"initial observation shape: \",obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "404YXrb_rjNP"
   },
   "outputs": [],
   "source": [
    "#--------value network (critic)----------\n",
    "class ValueNetwork(nn.Module):\n",
    "  def __init__(self,input_dim=768):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Linear(input_dim,256),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(256,1)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rA8anqmrrqam"
   },
   "outputs": [],
   "source": [
    "#----PPO Trainer----\n",
    "class PPOTrainer:\n",
    "  def __init__(self,policy_model,value_model,tokenizer,lr=1e-5,clip_eps=0.2):\n",
    "    self.policy=policy_model\n",
    "    self.value_net = value_model\n",
    "    self.tokenizer = tokenizer\n",
    "    self.clip_eps = clip_eps\n",
    "    self.optimizer = torch.optim.Adam(\n",
    "        list(self.policy.parameters()) + list(self.value_net.parameters()),\n",
    "        lr = lr\n",
    "    )\n",
    "\n",
    "  def compute_log_probs(self,inputs,actions):\n",
    "    with torch.no_grad():\n",
    "      logits = self.policy(**inputs).logits\n",
    "    dist = torch.distributions.Categorical(logits=logits[:,-1,:])\n",
    "    return dist.log_prob(actions) , dist\n",
    "\n",
    "  def train(self,observations , texts,actions,rewards,old_log_probs):\n",
    "    obs_tensor = torch.tensor(observations,dtype=torch.float32)\n",
    "    actions = torch.tensor(actions)\n",
    "    actions = actions.to(\"cuda\")\n",
    "    rewards = torch.tensor(rewards,dtype=torch.float32)\n",
    "    old_log_probs = torch.stack(old_log_probs)\n",
    "\n",
    "    # estimate value\n",
    "    values = self.value_net(obs_tensor).squeeze()\n",
    "    advantages = rewards - values.detach()\n",
    "    advantages = advantages.to(\"cuda\")\n",
    "\n",
    "    # get new log probs\n",
    "    inputs = tokenizer(texts , return_tensors=\"pt\",padding=True,truncation=True).to(\"cuda\")\n",
    "    logits = self.policy(**inputs).logits\n",
    "    dists = torch.distributions.Categorical(logits=logits[:,-1,:])\n",
    "    new_log_probs = dists.log_prob(actions)\n",
    "\n",
    "    # PPO Loss\n",
    "    ratios = torch.exp(new_log_probs-old_log_probs)\n",
    "    ratios = ratios.to(\"cuda\")\n",
    "    surr1 = ratios*advantages\n",
    "    surr2 = torch.clamp(ratios,1-self.clip_eps,1+self.clip_eps)*advantages\n",
    "    policy_loss = -torch.min(surr1,surr2).mean()\n",
    "    print(f\"policy_loss : {policy_loss}\")\n",
    "    value_loss = F.mse_loss(values , rewards)\n",
    "    print(f\"value_loss : {value_loss}\")\n",
    "    loss = policy_loss + value_loss\n",
    "    print(f\"loss : {loss}\")\n",
    "\n",
    "    # update\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fFxeIKXkrxBT"
   },
   "outputs": [],
   "source": [
    "env = ChatbotEnv()\n",
    "value_net = ValueNetwork()\n",
    "trainer = PPOTrainer(policy_model , value_net,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0z6xcORhsBgw",
    "outputId": "3c8e67c9-381d-4841-8fcb-ae32a35dd5c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_loss : 184.87246704101562\n",
      "value_loss : 0.04459132254123688\n",
      "loss : 184.91705322265625\n",
      "Epoch 0 done\n",
      "------------------\n",
      "policy_loss : 224.5618896484375\n",
      "value_loss : 0.0593060627579689\n",
      "loss : 224.62120056152344\n",
      "Epoch 1 done\n",
      "------------------\n",
      "policy_loss : 8.639435768127441\n",
      "value_loss : 0.049078211188316345\n",
      "loss : 8.68851375579834\n",
      "Epoch 2 done\n",
      "------------------\n",
      "policy_loss : 18.305387496948242\n",
      "value_loss : 0.05003967881202698\n",
      "loss : 18.355426788330078\n",
      "Epoch 3 done\n",
      "------------------\n",
      "policy_loss : 133.70272827148438\n",
      "value_loss : 0.03206789866089821\n",
      "loss : 133.73480224609375\n",
      "Epoch 4 done\n",
      "------------------\n",
      "policy_loss : 176.78652954101562\n",
      "value_loss : 0.03516224026679993\n",
      "loss : 176.82168579101562\n",
      "Epoch 5 done\n",
      "------------------\n",
      "policy_loss : 0.09778323769569397\n",
      "value_loss : 0.013048294000327587\n",
      "loss : 0.11083152890205383\n",
      "Epoch 6 done\n",
      "------------------\n",
      "policy_loss : 3.5736701488494873\n",
      "value_loss : 0.03998035565018654\n",
      "loss : 3.6136505603790283\n",
      "Epoch 7 done\n",
      "------------------\n",
      "policy_loss : 40.6852912902832\n",
      "value_loss : 0.029066449031233788\n",
      "loss : 40.714359283447266\n",
      "Epoch 8 done\n",
      "------------------\n",
      "policy_loss : 2.2577242851257324\n",
      "value_loss : 0.03425448760390282\n",
      "loss : 2.2919788360595703\n",
      "Epoch 9 done\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# -- Training Loop --\n",
    "for epoch in range(10):\n",
    "\n",
    "  observations = []\n",
    "  texts = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  old_log_probs = []\n",
    "\n",
    "  # rollouts\n",
    "  obs = env.reset()\n",
    "  for _ in range(100):\n",
    "    #if not env.conversation_history:\n",
    "     # text_input =  np.zeros(env.observation_space.shape,dtype=np.float32)\n",
    "    text_input = \"\".join(env.conversation_history)  or \"hello\"\n",
    "    inputs = tokenizer(text_input,return_tensors=\"pt\",truncation=True).to(\"cuda\")\n",
    "    logits = policy_model(**inputs).logits\n",
    "    dist = torch.distributions.Categorical(logits=logits[:,-1,:])\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    obs , reward, done, info = env.step(action.item())\n",
    "\n",
    "    observations.append(obs)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    texts.append(text_input)\n",
    "    old_log_probs.append(log_prob)\n",
    "\n",
    "    if done :\n",
    "      obs = env.reset()\n",
    "\n",
    "  trainer.train(observations,texts,actions,rewards,old_log_probs)\n",
    "  print(f\"Epoch {epoch} done\\n------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "q3PfwRX3sEVU"
   },
   "outputs": [],
   "source": [
    "torch.save(policy_model, \"DialoGPT_Train_With_PPO1.pth\")"
   ]
  }
 ],
 "metadata": {
  "cells": [],
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  },
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "name": "python",
    "version": "3.10.12"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
