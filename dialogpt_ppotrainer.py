# -*- coding: utf-8 -*-
"""dialoGPT_PPOTrainer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pxE3oxVMx-1BrOOeCKEwIctRKwpSFhvA
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
from transformers import AutoModelForCausalLM , AutoTokenizer
from gym import Env , spaces
import numpy as np

model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
policy_model=AutoModelForCausalLM.from_pretrained(model_name)

tokenizer.pad_token = tokenizer.eos_token

print(tokenizer.vocab_size)
print(policy_model.config.hidden_size)

policy_model=policy_model.to("cuda")

class ChatbotEnv(Env):
  def __init__(self):
    super(ChatbotEnv,self).__init__()
    self.tokenizer = tokenizer
    self.model=policy_model
    self.action_space=spaces.Discrete(self.tokenizer.vocab_size)
    self.observation_space=spaces.Box(low=np.inf,high=np.inf,shape=(self.model.config.hidden_size,),dtype=np.float32)
    self.conversation_history=[]

  def reset(self):
    self.conversation_history =[]
    return self._get_observation()

  def step(self,action):
    response = self._decode_action(action)
    self.conversation_history.append(response)
    reward = self._calculate_reward(response)
    done = len(self.conversation_history) >=5
    obs = self._get_observation()
    return obs , reward, done, {}

  def _decode_action(self,action_token_id):
    return self.tokenizer.decode([action_token_id],skip_special_tokens=True)

  def _get_observation(self):
    if not self.conversation_history:
      return np.zeros(self.observation_space.shape,dtype=np.float32)
    text = " ".join(self.conversation_history)
    inputs = self.tokenizer(text,return_tensors="pt",truncation=True,max_length=512).to("cuda")
    with torch.no_grad():
      outputs = policy_model.transformer(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze(0).detach().cpu().numpy().astype(np.float32).flatten()

  def _calculate_reward(self,response):
    return len(response.split()) / 10.0

env_sample = ChatbotEnv()
obs = env_sample.reset()
print("initial observation shape: ",obs.shape)

#--------value network (critic)----------
class ValueNetwork(nn.Module):
  def __init__(self,input_dim=768):
    super().__init__()
    self.net = nn.Sequential(
        nn.Linear(input_dim,256),
        nn.Tanh(),
        nn.Linear(256,1)
    )

  def forward(self,x):
    return self.net(x)

#----PPO Trainer----
class PPOTrainer:
  def __init__(self,policy_model,value_model,tokenizer,lr=1e-5,clip_eps=0.2):
    self.policy=policy_model
    self.value_net = value_model
    self.tokenizer = tokenizer
    self.clip_eps = clip_eps
    self.optimizer = torch.optim.Adam(
        list(self.policy.parameters()) + list(self.value_net.parameters()),
        lr = lr
    )

  def compute_log_probs(self,inputs,actions):
    with torch.no_grad():
      logits = self.policy(**inputs).logits
    dist = torch.distributions.Categorical(logits=logits[:,-1,:])
    return dist.log_prob(actions) , dist

  def train(self,observations , texts,actions,rewards,old_log_probs):
    obs_tensor = torch.tensor(observations,dtype=torch.float32)
    actions = torch.tensor(actions)
    actions = actions.to("cuda")
    rewards = torch.tensor(rewards,dtype=torch.float32)
    old_log_probs = torch.stack(old_log_probs)

    # estimate value
    values = self.value_net(obs_tensor).squeeze()
    advantages = rewards - values.detach()
    advantages = advantages.to("cuda")

    # get new log probs  
    inputs = tokenizer(texts , return_tensors="pt",padding=True,truncation=True).to("cuda")
    logits = self.policy(**inputs).logits
    dists = torch.distributions.Categorical(logits=logits[:,-1,:])
    new_log_probs = dists.log_prob(actions)

    # PPO Loss
    ratios = torch.exp(new_log_probs-old_log_probs)
    ratios = ratios.to("cuda")
    surr1 = ratios*advantages
    surr2 = torch.clamp(ratios,1-self.clip_eps,1+self.clip_eps)*advantages
    policy_loss = -torch.min(surr1,surr2).mean()
    print(f"policy_loss : {policy_loss}")
    value_loss = F.mse_loss(values , rewards)
    print(f"value_loss : {value_loss}")
    loss = policy_loss + value_loss
    print(f"loss : {loss}")

    # update
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()

env = ChatbotEnv()
value_net = ValueNetwork()
trainer = PPOTrainer(policy_model , value_net,tokenizer)

# -- Training Loop --
for epoch in range(10):

  observations = []
  texts = []
  actions = []
  rewards = []
  old_log_probs = []

  # rollouts
  obs = env.reset()
  for _ in range(100):
    #if not env.conversation_history:
     # text_input =  np.zeros(env.observation_space.shape,dtype=np.float32)
    text_input = "".join(env.conversation_history)  or "hello"
    inputs = tokenizer(text_input,return_tensors="pt",truncation=True).to("cuda")
    logits = policy_model(**inputs).logits
    dist = torch.distributions.Categorical(logits=logits[:,-1,:])
    action = dist.sample()
    log_prob = dist.log_prob(action)

    obs , reward, done, info = env.step(action.item())

    observations.append(obs)
    actions.append(action)
    rewards.append(reward)
    texts.append(text_input)
    old_log_probs.append(log_prob)

    if done :
      obs = env.reset()

  trainer.train(observations,texts,actions,rewards,old_log_probs)
  print(f"Epoch {epoch} done")

torch.save(policy_model, "DialoGPT_Train_With_PPO1.pth")

