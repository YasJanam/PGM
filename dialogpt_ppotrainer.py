# -*- coding: utf-8 -*-
"""dialoGPT_PPOTrainer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pxE3oxVMx-1BrOOeCKEwIctRKwpSFhvA
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
from transformers import AutoModelForCausalLM , AutoTokenizer
from gym import Env , spaces
import numpy as np

model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
policy_model=AutoModelForCausalLM.from_pretrained(model_name)

tokenizer.pad_token = tokenizer.eos_token

print(tokenizer.vocab_size)
print(policy_model.config.hidden_size)

policy_model=policy_model.to("cuda")

class ChatbotEnv(Env):
  def __init__(self):
    super(ChatbotEnv,self).__init__()
    self.tokenizer = tokenizer
    self.model=policy_model
    self.action_space=spaces.Discrete(self.tokenizer.vocab_size)
    self.observation_space=spaces.Box(low=np.inf,high=np.inf,shape=(self.model.config.hidden_size,),dtype=np.float32)
    self.conversation_history=[]

  def reset(self):
    self.conversation_history =[]
    return self._get_observation()

  def step(self,action):
    #print("call step --> ")
    response = self._decode_action(action)
    self.conversation_history.append(response)
    reward = self._calculate_reward(response)
    done = len(self.conversation_history) >=5
    obs = self._get_observation()
    return obs , reward, done, {}

  def _decode_action(self,action_token_id):
    return self.tokenizer.decode([action_token_id],skip_special_tokens=True)

  def _get_observation(self):
   # print("call _get_observation --> ")
    if not self.conversation_history:
      return np.zeros(self.observation_space.shape,dtype=np.float32)
    text = " ".join(self.conversation_history)
    inputs = self.tokenizer(text,return_tensors="pt",truncation=True,max_length=512).to("cuda")
    with torch.no_grad():
      outputs = policy_model.transformer(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze(0).detach().cpu().numpy().astype(np.float32).flatten()

  def _calculate_reward(self,response):
    return len(response.split()) / 10.0

env_sample = ChatbotEnv()
obs = env_sample.reset()
print("initial observation shape: ",obs.shape)

#--------value network (critic)----------
class ValueNetwork(nn.Module):
  def __init__(self,input_dim=768):
    super().__init__()
    self.net = nn.Sequential(
        nn.Linear(input_dim,256),
        nn.Tanh(),
        nn.Linear(256,1)
    )

  def forward(self,x):
    return self.net(x)

#----PPO Trainer----
class PPOTrainer:
  def __init__(self,policy_model,value_model,tokenizer,lr=1e-5,clip_eps=0.2):
    self.policy=policy_model
    self.value_net = value_model
    self.tokenizer = tokenizer
    self.clip_eps = clip_eps
    self.optimizer = torch.optim.Adam(
        list(self.policy.parameters()) + list(self.value_net.parameters()),
        lr = lr
    )

  def compute_log_probs(self,inputs,actions):
    with torch.no_grad():
      logits = self.policy(**inputs).logits
    dist = torch.distributions.Categorical(logits=logits[:,-1,:])
    return dist.log_prob(actions) , dist

  def train(self,observations , texts,actions,rewards,old_log_probs):
    print("call train --> ")
    #print("obs to tensor : ")
    obs_tensor = torch.tensor(observations,dtype=torch.float32)
    #print("actions to tensor :")
    actions = torch.tensor(actions)
    actions = actions.to("cuda")
    #print("tewards to tensor : ")
    rewards = torch.tensor(rewards,dtype=torch.float32)
    #print("old_log_probs --> stack :")
    old_log_probs = torch.stack(old_log_probs)

    # estimate value
    #print("value_net :")
    values = self.value_net(obs_tensor).squeeze()
    #print("calculate advantages :")
    advantages = rewards - values.detach()
    advantages = advantages.to("cuda")

    # get new log probs
    #print("get new log probs :")
    #print("inputs :")
    inputs = tokenizer(texts , return_tensors="pt",padding=True,truncation=True).to("cuda")
    #print("logits :")
    logits = self.policy(**inputs).logits
    #print("softmax :")
    dists = torch.distributions.Categorical(logits=logits[:,-1,:])
    #print("get new log probs:")
    new_log_probs = dists.log_prob(actions)

    # PPO Loss
    #print("PPO LOSS :")
    ratios = torch.exp(new_log_probs-old_log_probs)
    #print(f"ratios : {ratios}")
    ratios = ratios.to("cuda")
    surr1 = ratios*advantages
    #print(f"surr1 : {surr1}")
    surr2 = torch.clamp(ratios,1-self.clip_eps,1+self.clip_eps)*advantages
    #print(f"surr2 : {surr2}")
    policy_loss = -torch.min(surr1,surr2).mean()
    print(f"policy_loss : {policy_loss}")
    value_loss = F.mse_loss(values , rewards)
    print(f"value_loss : {value_loss}")
    loss = policy_loss + value_loss
    print(f"loss : {loss}")

    # update
    #print("update")
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()

env = ChatbotEnv()
value_net = ValueNetwork()
trainer = PPOTrainer(policy_model , value_net,tokenizer)

# -- Training Loop --
for epoch in range(10):

  observations = []
  texts = []
  actions = []
  rewards = []
  old_log_probs = []

  # rollouts
  obs = env.reset()
  for _ in range(100):
    #if not env.conversation_history:
     # text_input =  np.zeros(env.observation_space.shape,dtype=np.float32)
    text_input = "".join(env.conversation_history)  or "hello"
    inputs = tokenizer(text_input,return_tensors="pt",truncation=True).to("cuda")
    logits = policy_model(**inputs).logits
    dist = torch.distributions.Categorical(logits=logits[:,-1,:])
    action = dist.sample()
    log_prob = dist.log_prob(action)

    obs , reward, done, info = env.step(action.item())

    observations.append(obs)
    actions.append(action)
    rewards.append(reward)
    texts.append(text_input)
    old_log_probs.append(log_prob)

    if done :
      obs = env.reset()

  trainer.train(observations,texts,actions,rewards,old_log_probs)
  print(f"Epoch {epoch} done")

torch.save(policy_model, "DialoGPT_Train_With_PPO1.pth")

"""# دستگرمی"""

pip install torchviz

pip install torchinfo

from torchinfo import summary

summary(policy_model, input_size=(1,128), depth=2)

from torchviz import make_dot

inputs = tokenizer("i have a book", return_tensors="pt").to("cuda")
outputs = policy_model(**inputs)
make_dot(outputs.logits, params=dict(policy_model.named_parameters())).render("model_graph", format="png")

for name , module in policy_model.named_modules():
  print(name , "=>" , module.__class__.__name__)

print(policy_model.config)

import matplotlib.pyplot as plt

for name , param in policy_model.named_parameters():
  if param.grad is not None:
    print("------------------------")
    print(f"parameter : {name} --->   gradient norm : {param.grad.norm().item()}")

for name , param in policy_model.named_parameters():
  plt.figure()
  plt.hist(param.detach().cpu().numpy().flatten(), bins=50)
  plt.title(f"parameter Distribution : {name}")
  plt.show()